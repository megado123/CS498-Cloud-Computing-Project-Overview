{"cells":[{"cell_type":"code","source":["# Import Necessary Libraries\nimport pydocumentdb\nfrom pydocumentdb import document_client\nfrom pydocumentdb import documents\nimport datetime\n\n# Configuring the connection policy (allowing for endpoint discovery)\nconnectionPolicy = documents.ConnectionPolicy()\nconnectionPolicy.EnableEndpointDiscovery \nconnectionPolicy.PreferredLocations = [\"Central US\", \"East US 2\", \"Southeast Asia\", \"Western Europe\",\"Canada Central\"]\n\n# Set keys to connect to Cosmos DB \nmasterKey = 'ZvHF3rrP2qaNQ6Yri5rjkKpVTBPNTq2iJ1k28EMBSne4DkSXscNTJzRGG4QZktBfREgfOmSqKCzYGPzqDqZ3sw==' \nhost = 'https://cs489cosmossql.documents.azure.com:443/'\nclient = document_client.DocumentClient(host, {'masterKey': masterKey}, connectionPolicy)\n\n# Configure Database and Collections\ndatabaseId = 'cralSpectra'\ncollectionId = 'experimentalSpectra'\n\n# Configurations the Cosmos DB client will use to connect to the database and collection\ndbLink = 'dbs/' + databaseId\ncollLink = dbLink + '/colls/' + collectionId\n\n# Set query parameter\n#querystr = \"SELECT c.City FROM c WHERE c.State='WA'\"\nquerystr = \"SELECT * FROM experimentalSpectra f WHERE f.Compound = 'Unknown'\"\n\n# Query documents\nquery = client.QueryDocuments(collLink, querystr, options=None, partition_key=None)\n\n# Query for partitioned collections\n# query = client.QueryDocuments(collLink, querystr, options= { 'enableCrossPartitionQuery': True }, partition_key=None)\n\n# Push into list `elements`\nelements = list(query)\n\nbcontinue = False\n# Create `df` Spark DataFrame from `elements` Python list\nif len(elements) > 0:\n  bcontinue = True\n  df_exp = spark.createDataFrame(elements)\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["if bcontinue == True:\n    a_exp = df_exp.rdd.map(lambda p: p.PeakList.split(';')).collect()\n    b_exp = df_exp.rdd.map(lambda x: x.Compound).collect()\n    id_exp = df_exp.rdd.map(lambda x: x._self).collect()\n    bcontinue = True;"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["if bcontinue == True:\n    import pandas as pd\n    from pyspark.sql import functions as F\n    from pyspark.sql.types import *\n\n    mz_arr = []\n    intensity_arr = []\n    sample_name = []\n    sample_id = []\n    \n    print(len(a_exp))\n    \n    for i in range(0,len(a_exp)):\n    #print(i)\n        for item in a_exp[i]:\n            mz, intensity = item.split(':')\n            mz_arr.append(int(mz))\n            intensity_arr.append(int(intensity))\n            #print(id_exp[i])\n            sample_id.append(id_exp[i])\n        \n        \n    #added so during pivot, data will be all in vectors of sample length\n    for i in range(0, 1761):\n        mz_arr.append(i)\n        intensity_arr.append(0)\n        sample_name.append(\"reference\")\n        sample_id.append(\"none\")\n  \n    df_exp = pd.DataFrame({'sample_id': sample_id,'mz':mz_arr, 'intensity':intensity_arr})\n\n\n    #Create PySpark DataFrame Schema\n    p_schema = StructType([StructField('intensity',IntegerType(),True),StructField('mz',IntegerType(),True),StructField('sample_id',StringType(),True)])\n\n\n    #Create Spark DataFrame from Pandas\n    df_spark2_exp = sqlContext.createDataFrame(df_exp, p_schema)\n\n    reshaped_df_exp = df_spark2_exp.groupby('sample_id').pivot('mz').max('intensity').fillna(0).filter(df_spark2_exp.sample_id != \"none\")\n    \n    #reshaped_df_exp.show()\n\n    #reshaped_df_exp.write.saveAsTable(\"spectra_pivotted_exp7\")\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#collect took 16.32 seconds\nif bcontinue == True:\n    ref_samplenames = spark.sql(\"SELECT sample_name FROM spectra_pivotted\").collect()\n    #ref_samplenames_collect = spark.sql(\"SELECT sample_name FROM spectra_pivotted\").collect()\n    ref_intensity = spark.sql(\"SELECT * FROM spectra_pivotted\").drop(\"sample_name\")\n    bcontinue = True"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["if bcontinue == True:\n    import math\n    import numpy as np\n    from numpy import array\n    mzs = np.arange(1761)\n    arr_mzs = array(mzs)\n    score = []\n    j = 0\n    exp_intensity = reshaped_df_exp.drop(\"sample_id\").collect()\n    exp_id = reshaped_df_exp.select(\"sample_id\").collect()\n    \n    def dotproduct(row):\n        result = np.multiply(np.dot(row, arr_mzs), exp_demoninator)\n        return result\n    \n    def numerator(row):\n        result = np.dot(np.sqrt(np.multiply(row, array(exp_intensity))),arr_mzs)\n        result = result * result\n        return result\n\n    exp_demoninator = np.dot(exp_intensity,arr_mzs )\n    #print(\"exp_demon\")\n    #print(exp_demoninator)\n    #denominators = np.multiply(ref_demoninator, exp_demoninator)\n    temp_denom = ref_intensity.rdd.map(dotproduct)\n    denominators = temp_denom.collect()\n    #print(\"denominators\")\n    #print(denominators[0])\n    numerators = array(ref_intensity.rdd.map(numerator).collect())\n    #print(len(denominators))\n    #print(denominators[0])\n    #print(len(numerators))\n    #print(numerators[0])\n    val = np.divide(numerators, denominators)\n    #print(type(val))\n    #print(len(val))\n    #print(val[0])    \n    bcontinue = True\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["if bcontinue == True:\n    new_dict = {}  \n    for j in range(0, len(exp_intensity)):\n        arr_sample_max_ids = val[:,j]\n        sample_idx = np.argpartition(arr_sample_max_ids, -5)[-5:]\n        #sample_idx = np.argpartition(val, -5)[-5:]\n        print(sample_idx)\n        strResult = (np.take(ref_samplenames, sample_idx))\n        strId = exp_id[j][0]\n        new_dict.update({strId: strResult})\n\n    print(new_dict)\n    bcontinue = True"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Import Necessary Libraries\nif bcontinue == True:\n    import pydocumentdb\n    from pydocumentdb import document_client\n    from pydocumentdb import documents\n    import datetime\n\n    # Configuring the connection policy (allowing for endpoint discovery)\n    connectionPolicy = documents.ConnectionPolicy()\n    connectionPolicy.EnableEndpointDiscovery \n    connectionPolicy.PreferredLocations = [\"Central US\", \"East US 2\", \"Southeast Asia\", \"Western Europe\",\"Canada Central\"]\n\n    # Set keys to connect to Cosmos DB \n    masterKey = 'ZvHF3rrP2qaNQ6Yri5rjkKpVTBPNTq2iJ1k28EMBSne4DkSXscNTJzRGG4QZktBfREgfOmSqKCzYGPzqDqZ3sw==' \n    host = 'https://cs489cosmossql.documents.azure.com:443/'\n    client = document_client.DocumentClient(host, {'masterKey': masterKey}, connectionPolicy)\n\n    # Configure Database and Collections\n    databaseId = 'cralSpectra'\n    collectionId = 'experimentalSpectra'\n\n    # Configurations the Cosmos DB client will use to connect to the database and collection\n    dbLink = 'dbs/' + databaseId\n    collLink = dbLink + '/colls/' + collectionId\n\n    for key, value in new_dict.items():\n        id = key\n        # use a SQL based query to get a bunch of documents\n        querystr = \"SELECT * FROM experimentalSpectra f WHERE f._self = '\" + id + \"'\"\n\n        # Query documents\n        query = client.QueryDocuments(collLink, querystr, options=None, partition_key=None)\n\n        # Query for partitioned collections\n        # query = client.QueryDocuments(collLink, querystr, options= { 'enableCrossPartitionQuery': True }, partition_key=None)\n\n        # Push into list `elements`\n        elements = list(query)\n        print(len(elements))\n        doc= elements[0]\n\n        doc[\"Compound\"] = ' '.join(value)\n\n        # Get the document link from attribute `_self`\n        doc_link = doc['_self']\n        # Modify the document\n        # Replace the document via document link\n        client.ReplaceDocument(doc_link, doc)\n    "],"metadata":{},"outputs":[],"execution_count":7}],"metadata":{"name":"002_GetCosmosData parralized","notebookId":1907784916970350},"nbformat":4,"nbformat_minor":0}
